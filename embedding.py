# embedding.py

from typing import List, Dict, Tuple
from openai import OpenAI
import time


# <--- MODIFIED: è¿”å›å€¼å¢åŠ ä¸€ä¸ªæ•´æ•°ç”¨äºè®°å½•token ---
def _generate_embeddings_for_texts(texts: List[str], config: Dict, item_type: str = "items") -> Tuple[
    List[list[float] | None], int]:
    """
    ä¸€ä¸ªé€šç”¨çš„ã€æ ¸å¿ƒçš„å‘é‡ç”Ÿæˆå‡½æ•°ï¼Œæ¥æ”¶ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨ã€‚
    ç°åœ¨è¿”å›ä¸€ä¸ªå…ƒç»„ï¼š(å‘é‡åˆ—è¡¨, æ€»tokenæ¶ˆè€—)ã€‚
    """
    api_key = config.get('api_key')
    base_url = config.get('base_url')
    model_name = config.get('model_name')
    dimensions = config.get('dimensions')
    max_batch_size = config.get('max_batch_size', 25)

    total_tokens_used = 0  # <--- NEW: åˆå§‹åŒ–tokenè®¡æ•°å™¨

    if not all([api_key, base_url, model_name, dimensions]):
        print("âš ï¸  Warning: Embedding config is incomplete. Skipping embedding generation.")
        return [None] * len(texts), total_tokens_used

    try:
        client = OpenAI(api_key=api_key, base_url=base_url)
    except Exception as e:
        print(f"âŒ Error initializing OpenAI client: {e}. Skipping embedding generation.")
        return [None] * len(texts), total_tokens_used

    cleaned_texts = []
    original_indices = []
    for i, text in enumerate(texts):
        if text and not text.isspace():
            cleaned_texts.append(text)
            original_indices.append(i)
        else:
            print(f"   - Skipping empty or whitespace-only chunk at original index {i}.")

    if not cleaned_texts:
        print("âœ… No valid texts to embed in this batch.")
        return [None] * len(texts), total_tokens_used

    all_embeddings = []
    print(f"ğŸ§¬ Generating embeddings for {len(cleaned_texts)} valid {item_type} using '{model_name}'...")

    for i in range(0, len(cleaned_texts), max_batch_size):
        batch_texts = cleaned_texts[i:i + max_batch_size]
        try:
            print(f"   - Processing batch {i // max_batch_size + 1}/{-(-len(cleaned_texts) // max_batch_size)}...")
            response = client.embeddings.create(
                model=model_name,
                input=batch_texts,
                dimensions=dimensions
            )
            # <--- NEW: ç´¯åŠ ä»APIè¿”å›çš„tokenæ¶ˆè€— ---
            if response.usage:
                total_tokens_used += response.usage.total_tokens

            batch_embeddings = [embedding.embedding for embedding in response.data]
            all_embeddings.extend(batch_embeddings)
            time.sleep(0.1)

        except Exception as e:
            print(f"âŒ Error during API call for batch {i // max_batch_size + 1}: {e}")
            all_embeddings.extend([None] * len(batch_texts))

    final_results = [None] * len(texts)
    for i, embedding in enumerate(all_embeddings):
        original_idx = original_indices[i]
        final_results[original_idx] = embedding

    return final_results, total_tokens_used  # <--- MODIFIED: è¿”å›tokenæ€»æ•°


# --- ä¿®æ”¹æ‰€æœ‰è°ƒç”¨å¤„çš„å°è£…å‡½æ•°ä»¥å¤„ç†æ–°çš„è¿”å›å€¼ ---

def generate_chunk_embeddings(chunks: List[Dict], config: Dict) -> Tuple[List[Dict], int]:
    """ä¸ºæ–‡æœ¬å—åˆ—è¡¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼Œå¹¶è¿”å›tokenæ¶ˆè€—ã€‚"""
    texts_to_embed = [chunk['text'] for chunk in chunks]
    embeddings, tokens_used = _generate_embeddings_for_texts(texts_to_embed, config, item_type="chunks")

    for chunk, embedding in zip(chunks, embeddings):
        chunk['embedding'] = embedding

    print("âœ… Chunk embedding generation complete.")
    return chunks, tokens_used


def generate_entity_embeddings(entities: List[Dict], config: Dict) -> Tuple[List[Dict], int]:
    """ä¸ºå®ä½“åˆ—è¡¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼Œå¹¶è¿”å›tokenæ¶ˆè€—ã€‚"""
    texts_to_embed = [f"{e['entity_name']}\n{e['description']}" for e in entities]
    embeddings, tokens_used = _generate_embeddings_for_texts(texts_to_embed, config, item_type="entities")

    for entity, embedding in zip(entities, embeddings):
        entity['embedding'] = embedding

    print("âœ… Entity embedding generation complete.")
    return entities, tokens_used


def generate_relation_embeddings(relations: List[Dict], config: Dict) -> Tuple[List[Dict], int]:
    """ä¸ºå…³ç³»åˆ—è¡¨ç”Ÿæˆå‘é‡åµŒå…¥ï¼Œå¹¶è¿”å›tokenæ¶ˆè€—ã€‚"""
    texts_to_embed = [
        f"{r['keywords']}\t{r['source_name']}\n{r['target_name']}\n{r['description']}"
        for r in relations
    ]
    embeddings, tokens_used = _generate_embeddings_for_texts(texts_to_embed, config, item_type="relations")

    for relation, embedding in zip(relations, embeddings):
        relation['embedding'] = embedding

    print("âœ… Relation embedding generation complete.")
    return relations, tokens_used