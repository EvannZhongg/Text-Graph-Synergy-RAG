# fusion.py

import pandas as pd
from pathlib import Path
from typing import List, Dict
from embedding import generate_entity_embeddings, generate_relation_embeddings
from collections import Counter


def fuse_and_update_knowledge_base(
        all_new_entities: List[Dict],
        all_new_relations: List[Dict],
        all_new_chunks_df: pd.DataFrame,
        rag_space_path: Path,
        embedding_config: Dict
):
    """
    æ‰§è¡Œå…¨å±€çŸ¥è¯†èåˆï¼Œä¸ºå®ä½“å’Œå…³ç³»ç”Ÿæˆå‘é‡ï¼Œå¹¶æ›´æ–°æ‰€æœ‰ Parquet æ–‡ä»¶ã€‚
    æ­¤ç‰ˆæœ¬åŒ…å«æœ€ç»ˆçš„å¥å£®åˆå¹¶é€»è¾‘ï¼Œä»¥å¤„ç†å„ç§æ•°æ®é‡å¤æƒ…å†µã€‚
    """
    print("\nğŸ”— Starting global knowledge fusion...")
    rag_space_path.mkdir(exist_ok=True)

    entities_path = rag_space_path / "vdb_entities.parquet"
    relations_path = rag_space_path / "vdb_relationships.parquet"
    chunks_path = rag_space_path / "vdb_chunks.parquet"

    # --- 1. åŠ è½½æˆ–åˆ›å»ºå…¨å±€æ•°æ® ---
    try:
        global_entities_df = pd.read_parquet(entities_path)
    except FileNotFoundError:
        global_entities_df = pd.DataFrame(
            columns=['entity_id', 'entity_name', 'entity_type', 'description', 'source_chunk_ids', 'degree',
                     'embedding'])

    try:
        global_relations_df = pd.read_parquet(relations_path)
    except FileNotFoundError:
        global_relations_df = pd.DataFrame(
            columns=['relation_id', 'source', 'target', 'keywords', 'description', 'source_chunk_ids', 'degree',
                     'embedding'])

    try:
        global_chunks_df = pd.read_parquet(chunks_path)
    except FileNotFoundError:
        global_chunks_df = pd.DataFrame(columns=['chunk_id', 'text', 'token_count', 'embedding'])

    # --- 2. èåˆå®ä½“ ---
    print(f"   - Fusing {len(all_new_entities)} new entities...")
    new_entities_df = pd.DataFrame(all_new_entities)

    # åˆå¹¶æ–°æ—§å®ä½“
    combined_entities_df = pd.concat([global_entities_df, new_entities_df], ignore_index=True)

    if not combined_entities_df.empty:
        # å®šä¹‰èšåˆå‡½æ•°
        agg_funcs = {
            'entity_id': 'first',
            'entity_type': lambda x: Counter(x).most_common(1)[0][0],
            'description': lambda x: max(x, key=len),
            # <--- MODIFIED: åœ¨ sum ä¹‹å‰ï¼Œä½¿ç”¨ apply(list) å¼ºåˆ¶è½¬æ¢ç±»å‹ ---
            'source_chunk_ids': lambda x: list(set(sum(x.apply(list), []))),
            'degree': 'sum'
        }

        global_entities_df = combined_entities_df.groupby('entity_name').agg(agg_funcs).reset_index()
    else:
        global_entities_df = pd.DataFrame(columns=global_entities_df.columns)

    # --- 3. èåˆå…³ç³» ---
    print(f"   - Fusing {len(all_new_relations)} new relations...")
    new_relations_df = pd.DataFrame(all_new_relations)

    if not new_relations_df.empty or not global_relations_df.empty:
        for df in [global_relations_df, new_relations_df]:
            if not df.empty:
                df['key'] = df.apply(lambda row: tuple(sorted((str(row['source']), str(row['target'])))), axis=1)

        combined_relations_df = pd.concat([global_relations_df, new_relations_df], ignore_index=True)

        agg_funcs = {
            'relation_id': 'first',
            'source': 'first',
            'target': 'first',
            'keywords': lambda x: ", ".join(sorted(set(kw.strip() for kw_list in x for kw in kw_list.split(',')))),
            'description': lambda x: " | ".join(set(x)),
            # <--- MODIFIED: åœ¨ sum ä¹‹å‰ï¼Œä½¿ç”¨ apply(list) å¼ºåˆ¶è½¬æ¢ç±»å‹ ---
            'source_chunk_ids': lambda x: list(set(sum(x.apply(list), []))),
            'degree': 'sum'
        }

        global_relations_df = combined_relations_df.groupby('key').agg(agg_funcs).reset_index(drop=True)
    else:
        global_relations_df = pd.DataFrame(columns=global_relations_df.columns)

    if 'key' in global_relations_df.columns:
        global_relations_df = global_relations_df.drop(columns=['key'])

    # --- 4. ä¸ºæœ€ç»ˆçš„å…¨å±€å®ä½“å’Œå…³ç³»ç”Ÿæˆå‘é‡ ---
    if embedding_config.get('api_key'):
        # ä»…å¯¹å‘ç”Ÿå˜åŒ–æˆ–æ–°å¢çš„è¡Œè¿›è¡Œå‘é‡åŒ–ï¼Œä»¥æé«˜æ•ˆç‡
        # (ä¸ºç®€åŒ–ï¼Œè¿™é‡Œä»ç„¶å¯¹å…¨éƒ¨æ•°æ®è¿›è¡Œå‘é‡åŒ–ï¼Œæœªæ¥å¯ä¼˜åŒ–)
        if not global_entities_df.empty:
            entities_list = global_entities_df.to_dict('records')
            entities_with_embeddings = generate_entity_embeddings(entities_list, embedding_config)
            global_entities_df = pd.DataFrame(entities_with_embeddings)

        if not global_relations_df.empty:
            relations_list = global_relations_df.to_dict('records')
            relations_with_embeddings = generate_relation_embeddings(relations_list, embedding_config)
            global_relations_df = pd.DataFrame(relations_with_embeddings)

    # --- 5. ä¿å­˜èåˆåçš„å…¨å±€å®ä½“å’Œå…³ç³» ---
    if 'degree' in global_entities_df.columns:
        global_entities_df['degree'] = global_entities_df['degree'].fillna(0).astype(int)
    if 'degree' in global_relations_df.columns:
        global_relations_df['degree'] = global_relations_df['degree'].fillna(0).astype(int)

    global_entities_df.to_parquet(entities_path, index=False)
    global_relations_df.to_parquet(relations_path, index=False)
    print(f"ğŸ’¾ Global entities with embeddings saved to: {entities_path}")
    print(f"ğŸ’¾ Global relations with embeddings saved to: {relations_path}")

    # --- 6. æ›´æ–°å…¨å±€æ–‡æœ¬å— ---
    print(f"   - Updating global chunks with new data...")
    if 'embedding' not in global_chunks_df.columns:
        global_chunks_df['embedding'] = None
    global_chunks_df = pd.concat([global_chunks_df, all_new_chunks_df]).drop_duplicates(subset=['chunk_id'],
                                                                                        keep='last').reset_index(
        drop=True)

    # --- 7. æ˜ å°„å®ä½“å’Œå…³ç³»IDå›æ–‡æœ¬å— ---
    print(f"   - Mapping knowledge graph back to chunks...")
    # ... æ˜ å°„é€»è¾‘ä¿æŒä¸å˜ ...
    if 'entity_ids' not in global_chunks_df.columns:
        global_chunks_df['entity_ids'] = pd.Series([[] for _ in range(len(global_chunks_df))])
    if 'relation_ids' not in global_chunks_df.columns:
        global_chunks_df['relation_ids'] = pd.Series([[] for _ in range(len(global_chunks_df))])

    chunk_map = {chunk_id: {'entity_ids': [], 'relation_ids': []} for chunk_id in global_chunks_df['chunk_id']}

    for _, entity in global_entities_df.iterrows():
        sources = entity.get('source_chunk_ids', [])
        if isinstance(sources, list):
            for chunk_id in sources:
                if chunk_id in chunk_map:
                    chunk_map[chunk_id]['entity_ids'].append(entity['entity_id'])

    for _, relation in global_relations_df.iterrows():
        sources = relation.get('source_chunk_ids', [])
        if isinstance(sources, list):
            for chunk_id in sources:
                if chunk_id in chunk_map:
                    chunk_map[chunk_id]['relation_ids'].append(relation['relation_id'])

    map_df = pd.DataFrame.from_dict(chunk_map, orient='index').reset_index().rename(columns={'index': 'chunk_id'})

    if 'entity_ids' in global_chunks_df.columns:
        global_chunks_df = global_chunks_df.drop(columns=['entity_ids'])
    if 'relation_ids' in global_chunks_df.columns:
        global_chunks_df = global_chunks_df.drop(columns=['relation_ids'])

    global_chunks_df = global_chunks_df.merge(map_df, on='chunk_id', how='left')

    # --- 8. ä¿å­˜æœ€ç»ˆçš„å…¨å±€æ–‡æœ¬å—æ–‡ä»¶ ---
    global_chunks_df.to_parquet(chunks_path, index=False)
    print(f"ğŸ’¾ Global chunks with KG mapping saved to: {chunks_path}")

    print("âœ… Global knowledge fusion finished.")