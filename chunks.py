import tiktoken
import re
from typing import List, Dict

# åŠ è½½ç¼–ç å™¨
ENCODING = tiktoken.get_encoding("cl100k_base")


def _get_token_count(text: str) -> int:
    """è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    return len(ENCODING.encode(text))


def _preprocess_text(text: str) -> str:
    """
    æ–‡æœ¬é¢„å¤„ç†å™¨ï¼šåœ¨åˆ†å—å‰æ¸…æ´—æ‰ PDF è½¬ Markdown æ’å…¥çš„å›¾ç‰‡å ä½ç¬¦å’Œ Markdown å›¾ç‰‡é“¾æ¥ã€‚
    """
    # 1. ç§»é™¤ PDF è½¬ Markdown ç”Ÿæˆçš„å›¾ç‰‡å ä½ç¬¦ï¼Œä¾‹å¦‚ <!-- image -->
    cleaned_text = re.sub(r'<!--\s*image\s*-->', '', text)

    # 2. ç§»é™¤æ‰€æœ‰ Markdown å›¾ç‰‡é“¾æ¥ï¼Œä¾‹å¦‚ ![alt](url)
    cleaned_text = re.sub(r'\n?!\[.*?\]\(.*?\)\n?', '', cleaned_text)

    return cleaned_text.strip()


def chunk_text_fixed_size(
        text: str,
        file_hash: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 150
) -> List[Dict]:
    """å›ºå®šå¤§å°çš„åˆ†å—ç­–ç•¥ (æ­¤å‡½æ•°ä¸å˜)"""
    if not text:
        return []
    tokens = ENCODING.encode(text)
    chunks = []
    step = chunk_size - chunk_overlap
    for i in range(0, len(tokens), step):
        chunk_tokens = tokens[i: i + chunk_size]
        chunk_text = ENCODING.decode(chunk_tokens)
        chunk_id = f"{file_hash}_{len(chunks)}"
        chunks.append({
            "chunk_id": chunk_id,
            "text": chunk_text,
            "token_count": len(chunk_tokens)
        })
    print(f"âœ… 'fixed' ç­–ç•¥åˆ†å—å®Œæˆï¼Œå…±è®¡ {len(chunks)} ä¸ªå—ã€‚")
    return chunks


def chunk_text_semantic(
        text: str,
        file_hash: str,
        target_size: int = 1000,
        overlap_target: int = 150
) -> List[Dict]:
    """é«˜çº§è¯­ä¹‰åˆ†å—ç­–ç•¥ V2 (ç°åœ¨å¤„ç†çš„æ˜¯å·²æ¸…æ´—è¿‡çš„æ–‡æœ¬)"""
    if not text:
        return []

    # å›¾ç‰‡é“¾æ¥å·²åœ¨é¢„å¤„ç†ä¸­è¢«ç§»é™¤ï¼Œç°åœ¨æˆ‘ä»¬åªæŒ‰æ ‡é¢˜å’Œæ®µè½åˆ‡åˆ†
    # æˆ‘ä»¬ä»¥ ## æˆ–æ›´é«˜çº§åˆ«çš„æ ‡é¢˜ä½œä¸ºä¸»è¦åˆ†éš”ç¬¦
    separators_regex = r'(\n##+\s.*)'
    blocks = re.split(separators_regex, text)

    structured_blocks = []
    i = 0
    while i < len(blocks):
        block = blocks[i].strip()
        if not block:
            i += 1
            continue
        if re.match(separators_regex, block) and i + 1 < len(blocks):
            next_block = blocks[i + 1].strip()
            structured_blocks.append(f"{block}\n\n{next_block}")
            i += 2
        else:
            structured_blocks.append(block)
            i += 1

    if not structured_blocks:
        return [
            {
                "chunk_id": f"{file_hash}_0",
                "text": text,
                "token_count": _get_token_count(text)
            }
        ] if text else []

    # åç»­çš„è´ªå¿ƒç®—æ³•ç»„åˆé€»è¾‘ä¿æŒä¸å˜
    base_chunks_of_blocks = []
    # ... (æ­¤éƒ¨åˆ†ä»£ç ä¸ä¸Šä¸€ç‰ˆå®Œå…¨ç›¸åŒï¼Œæ­¤å¤„çœç•¥ä»¥ä¿æŒç®€æ´)
    current_base_chunk = []
    current_tokens = 0
    for block in structured_blocks:
        block_tokens = _get_token_count(block)
        if current_tokens + block_tokens > target_size and current_base_chunk:
            base_chunks_of_blocks.append(current_base_chunk)
            current_base_chunk = [block]
            current_tokens = block_tokens
        else:
            current_base_chunk.append(block)
            current_tokens += block_tokens
    if current_base_chunk:
        base_chunks_of_blocks.append(current_base_chunk)

    final_chunks = []
    for i, base_chunk_blocks in enumerate(base_chunks_of_blocks):
        final_chunk_content_blocks = list(base_chunk_blocks)
        if i + 1 < len(base_chunks_of_blocks):
            next_base_chunk_blocks = base_chunks_of_blocks[i + 1]
            overlap_tokens = 0
            for block_to_add in next_base_chunk_blocks:
                block_to_add_tokens = _get_token_count(block_to_add)
                if block_to_add_tokens > overlap_target or (overlap_tokens + block_to_add_tokens) > overlap_target:
                    final_chunk_content_blocks.append(block_to_add)
                    break
                final_chunk_content_blocks.append(block_to_add)
                overlap_tokens += block_to_add_tokens
        final_text = "\n\n".join(final_chunk_content_blocks)
        chunk_id = f"{file_hash}_{len(final_chunks)}"
        final_chunks.append({
            "chunk_id": chunk_id,
            "text": final_text,
            "token_count": _get_token_count(final_text)
        })

    print(f"âœ… 'semantic' ç­–ç•¥åˆ†å—å®Œæˆï¼Œå…±è®¡ {len(final_chunks)} ä¸ªå—ã€‚")
    return final_chunks


def chunk_dispatcher(text: str, file_hash: str, config: Dict) -> List[Dict]:
    """
    åˆ†å—è°ƒåº¦å™¨ï¼šåœ¨åˆ†å‘ä»»åŠ¡å‰ï¼Œå…ˆè¿›è¡Œç»Ÿä¸€çš„æ–‡æœ¬é¢„å¤„ç†ã€‚
    """
    # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šåœ¨è¿™é‡Œç»Ÿä¸€è°ƒç”¨é¢„å¤„ç†å™¨ ---
    print("ğŸ§¹ Preprocessing text to remove image placeholders and links...")
    cleaned_text = _preprocess_text(text)

    strategy = config.get('strategy', 'fixed')

    if strategy == 'semantic':
        return chunk_text_semantic(
            cleaned_text,
            file_hash,
            target_size=config.get('semantic_target', 1000),
            overlap_target=config.get('semantic_overlap', 150)
        )
    elif strategy == 'fixed':
        return chunk_text_fixed_size(
            cleaned_text,
            file_hash,
            chunk_size=config.get('fixed_size', 1000),
            chunk_overlap=config.get('fixed_overlap', 150)
        )
    else:
        print(f"âš ï¸  Warning: Unknown chunking strategy '{strategy}'. Falling back to 'fixed'.")
        return chunk_text_fixed_size(cleaned_text, file_hash)