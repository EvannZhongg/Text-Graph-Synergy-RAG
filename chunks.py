import tiktoken
import re
from typing import List, Dict

# åŠ è½½ç¼–ç å™¨
ENCODING = tiktoken.get_encoding("cl100k_base")


def _get_token_count(text: str) -> int:
    """è¾…åŠ©å‡½æ•°ï¼šè®¡ç®—æ–‡æœ¬çš„ token æ•°é‡"""
    return len(ENCODING.encode(text))


def _preprocess_text(text: str) -> str:
    """
    æ–‡æœ¬é¢„å¤„ç†å™¨ï¼šåœ¨åˆ†å—å‰æ¸…æ´—æ‰ PDF è½¬ Markdown æ’å…¥çš„å›¾ç‰‡å ä½ç¬¦å’Œ Markdown å›¾ç‰‡é“¾æ¥ã€‚
    """
    # 1. ç§»é™¤ PDF è½¬ Markdown ç”Ÿæˆçš„å›¾ç‰‡å ä½ç¬¦ï¼Œä¾‹å¦‚ <!-- image -->
    cleaned_text = re.sub(r'<!--\s*image\s*-->', '', text)

    # 2. ç§»é™¤æ‰€æœ‰ Markdown å›¾ç‰‡é“¾æ¥ï¼Œä¾‹å¦‚ ![alt](url)
    cleaned_text = re.sub(r'\n?!\[.*?\]\(.*?\)\n?', '', cleaned_text)

    return cleaned_text.strip()


def chunk_text_fixed_size(
        text: str,
        file_hash: str,
        chunk_size: int = 1000,
        chunk_overlap: int = 150
) -> List[Dict]:
    """å›ºå®šå¤§å°çš„åˆ†å—ç­–ç•¥ (æ­¤å‡½æ•°ä¸å˜)"""
    if not text:
        return []
    tokens = ENCODING.encode(text)
    chunks = []
    step = chunk_size - chunk_overlap
    for i in range(0, len(tokens), step):
        chunk_tokens = tokens[i: i + chunk_size]
        chunk_text = ENCODING.decode(chunk_tokens)
        chunk_id = f"{file_hash}_{len(chunks)}"
        chunks.append({
            "chunk_id": chunk_id,
            "text": chunk_text,
            "token_count": len(chunk_tokens)
        })
    print(f"âœ… 'fixed' ç­–ç•¥åˆ†å—å®Œæˆï¼Œå…±è®¡ {len(chunks)} ä¸ªå—ã€‚")
    return chunks


def chunk_text_semantic(
        text: str,
        file_hash: str,
        target_size: int = 1000,
        overlap_target: int = 150,
        pre_context_limit: int = 50,
        hard_limit: int = 500
) -> List[Dict]:
    """
    é«˜çº§è¯­ä¹‰åˆ†å—ç­–ç•¥ V6ï¼šæœ€ç»ˆç‰ˆï¼Œé›†æˆäº†â€œè´ªå¿ƒä¸”ä¸è¶…æ ‡â€çš„å¸¸è§„é‡å é€»è¾‘ã€‚
    """
    if not text:
        return []

    # 1. ç»“æ„åŒ–æ‹†åˆ† (é€»è¾‘ä¸å˜)
    separators_regex = r'(\n##+\s.*)'
    blocks = re.split(separators_regex, text)
    structured_blocks = []
    i = 0
    while i < len(blocks):
        block = blocks[i].strip()
        if not block: i += 1; continue
        if re.match(separators_regex, block) and i + 1 < len(blocks):
            structured_blocks.append(f"{block}\n\n{blocks[i + 1].strip()}")
            i += 2
        else:
            structured_blocks.append(block)
            i += 1
    if not structured_blocks:
        return [{"chunk_id": f"{file_hash}_0", "text": text, "token_count": _get_token_count(text)}] if text else []

    # 2. æ„å»ºåŸºç¡€å— (é€»è¾‘ä¸å˜)
    base_chunks_of_blocks = []
    current_base_chunk = []
    current_tokens = 0
    for block in structured_blocks:
        block_tokens = _get_token_count(block)
        if block_tokens > target_size:
            if current_base_chunk:
                base_chunks_of_blocks.append(current_base_chunk)
            base_chunks_of_blocks.append([block])
            current_base_chunk = []
            current_tokens = 0
            continue
        if current_tokens + block_tokens > target_size and current_base_chunk:
            base_chunks_of_blocks.append(current_base_chunk)
            current_base_chunk = [block]
            current_tokens = block_tokens
        else:
            current_base_chunk.append(block)
            current_tokens += block_tokens
    if current_base_chunk:
        base_chunks_of_blocks.append(current_base_chunk)

    # 3. ä¸ºåŸºç¡€å—æ„å»ºæœ€ç»ˆåˆ†å—ï¼Œå¹¶åº”ç”¨æœ€ç»ˆç‰ˆçš„é‡å é€»è¾‘
    final_chunks = []
    for i, current_blocks in enumerate(base_chunks_of_blocks):
        final_chunk_content_blocks = list(current_blocks)

        if i > 0:
            previous_blocks = base_chunks_of_blocks[i - 1]
            last_unit_of_prev = previous_blocks[-1]
            last_unit_tokens = _get_token_count(last_unit_of_prev)

            overlap_blocks_to_prepend = []

            # è§„åˆ™ 1: ç¡¬æ€§ä¸Šé™
            if last_unit_tokens > hard_limit:
                pass
                # è§„åˆ™ 2: å¤§å•å…ƒç‰¹æ®Šè§„åˆ™
            elif last_unit_tokens > overlap_target:
                overlap_blocks_to_prepend.append(last_unit_of_prev)
                if len(previous_blocks) > 1:
                    pre_context_unit = previous_blocks[-2]
                    if _get_token_count(pre_context_unit) <= pre_context_limit:
                        overlap_blocks_to_prepend.insert(0, pre_context_unit)
            # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ–°çš„â€œè´ªå¿ƒä¸”ä¸è¶…æ ‡â€å¸¸è§„é‡å é€»è¾‘ ---
            else:
                current_overlap_tokens = 0
                for prev_block in reversed(previous_blocks):
                    prev_block_tokens = _get_token_count(prev_block)
                    # æ£€æŸ¥ä¸‹ä¸€ä¸ªå—åŠ è¿›æ¥æ˜¯å¦ä¼šâ€œè¶…æ ‡â€
                    if current_overlap_tokens + prev_block_tokens > overlap_target:
                        break  # å¦‚æœä¼šè¶…æ ‡ï¼Œåˆ™ç«‹å³åœæ­¢ï¼Œä¸æ·»åŠ è¿™ä¸ªå—

                    overlap_blocks_to_prepend.insert(0, prev_block)
                    current_overlap_tokens += prev_block_tokens

            final_chunk_content_blocks = overlap_blocks_to_prepend + final_chunk_content_blocks

        final_text = "\n\n".join(final_chunk_content_blocks)
        chunk_id = f"{file_hash}_{len(final_chunks)}"
        final_chunks.append({"chunk_id": chunk_id, "text": final_text, "token_count": _get_token_count(final_text)})

    print(f"âœ… 'semantic' ç­–ç•¥åˆ†å—å®Œæˆï¼Œå…±è®¡ {len(final_chunks)} ä¸ªå—ã€‚")
    return final_chunks


def chunk_dispatcher(text: str, file_hash: str, config: Dict) -> List[Dict]:
    """åˆ†å—è°ƒåº¦å™¨ (ç°åœ¨ä¼šä¼ é€’æ‰€æœ‰è¯­ä¹‰é…ç½®å‚æ•°)"""
    print("ğŸ§¹ Preprocessing text to remove image placeholders and links...")
    cleaned_text = _preprocess_text(text)

    strategy = config.get('strategy', 'fixed')

    if strategy == 'semantic':
        return chunk_text_semantic(
            cleaned_text,
            file_hash,
            target_size=config.get('semantic_target', 1000),
            overlap_target=config.get('semantic_overlap', 150),
            pre_context_limit=config.get('semantic_pre_context_limit', 50),
            hard_limit=config.get('semantic_hard_limit', 500)
        )
    elif strategy == 'fixed':
        return chunk_text_fixed_size(
            cleaned_text, file_hash,
            chunk_size=config.get('fixed_size', 1000),
            chunk_overlap=config.get('fixed_overlap', 150)
        )
    else:
        print(f"âš ï¸  Warning: Unknown chunking strategy '{strategy}'. Falling back to 'fixed'.")
        return chunk_text_fixed_size(cleaned_text, file_hash)